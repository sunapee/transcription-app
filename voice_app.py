import streamlit as st
import whisper
import tempfile
import os
import re
import gc
import torch

# ã‚¿ãƒ–ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨­å®š
st.set_page_config(
    page_title="éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚¢ãƒ—ãƒª",
    page_icon="ğŸ¤",
    layout="wide",
    initial_sidebar_state="expanded"
)

# UIã‚¿ã‚¤ãƒˆãƒ«
st.title("éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚¢ãƒ—ãƒª")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
st.sidebar.header("ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
st.sidebar.markdown("""
| ãƒ¢ãƒ‡ãƒ« | å¿…è¦ãƒ¡ãƒ¢ãƒª(æ¦‚ç®—) | å‡¦ç†é€Ÿåº¦ | ç²¾åº¦ |
|--------|--------------|---------|------|
| tiny   | 1GB          | æœ€é€Ÿ     | ä½   |
| base   | 1GB          | é€Ÿã„     | ä¸­ä½ |
| small  | 2GB          | ä¸­ç¨‹åº¦    | ä¸­   |
| medium | 5GB          | é…ã„     | ä¸­é«˜ |
| large  | 10GB         | æœ€é…     | é«˜   |
""")

# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹è¡¨ç¤ºé–¢æ•°
def display_memory_status():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        st.sidebar.text(f"GPUä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {allocated:.2f}GB / {reserved:.2f}GB")
    else:
        st.sidebar.text("GPUåˆ©ç”¨ä¸å¯ - CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­")

# Whisperãƒ¢ãƒ‡ãƒ«é¸æŠ
model_size = st.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼š", ["tiny", "base", "small", "medium", "large"])

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
device_option = "cpu"
if torch.cuda.is_available():
    device_option = st.radio("å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹:", ["cuda", "cpu"], horizontal=True)

# è¨€èªé¸æŠï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
language = st.selectbox("éŸ³å£°ã®è¨€èªã‚’é¸æŠï¼ˆè‡ªå‹•æ¤œå‡ºã®å ´åˆã¯ç©ºæ¬„ï¼‰ï¼š", ["", "è‹±èª", "ä¸­å›½èª", "æ—¥æœ¬èª"])

# è¨€èªã‚³ãƒ¼ãƒ‰ã®å¤‰æ›
language_dict = {
    "": None,
    "è‹±èª": "en",
    "ä¸­å›½èª": "zh",
    "æ—¥æœ¬èª": "ja"
}

# æ–‡å­—èµ·ã“ã—ã®é«˜åº¦ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³
with st.expander("é«˜åº¦ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³"):
    chunk_size = st.slider("éŸ³å£°ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚µã‚¤ã‚º(ç§’)", 5, 60, 30, 
                         help="é•·ã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å°ã•ãªåŒºé–“ã§å‡¦ç†ã—ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸›ã‚‰ã›ã¾ã™ãŒã€ç²¾åº¦ã«å½±éŸ¿ã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
    fp16_option = st.checkbox("åŠç²¾åº¦è¨ˆç®—ã‚’ä½¿ç”¨(FP16)", True, 
                            help="ã‚ªãƒ³ã«ã™ã‚‹ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åŠæ¸›ã§ãã¾ã™ãŒã€ç²¾åº¦ãŒã‚ãšã‹ã«ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆmp3, wav, m4aï¼‰", type=["mp3", "wav", "m4a"])

if uploaded_file is not None:
    st.audio(uploaded_file, format='audio/mp3')
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name
    
    with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å‰ã«ãƒ¡ãƒ¢ãƒªè§£æ”¾
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            model_options = {
                "device": device_option
            }
            
            if fp16_option and device_option == "cuda":
                model_options["fp16"] = True
            
            # Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model = whisper.load_model(model_size, **model_options)
            display_memory_status()
            
            # æ–‡å­—èµ·ã“ã—å‡¦ç†
            st.info("éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ä¸­ã§ã™...")
            transcribe_options = {
                "fp16": fp16_option and device_option == "cuda"
            }
            
            if language_dict[language] is not None:
                transcribe_options["language"] = language_dict[language]
            
            result = model.transcribe(tmp_path, **transcribe_options)
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del model
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ–‡ç« ã®æ•´å½¢
            sentences = re.split(r'(?<=[ã€‚ï¼.!?ï¼ï¼Ÿ])\s*', result["text"].strip())
            formatted_text = "\n".join(sentences)
            
            # æ–‡å­—èµ·ã“ã—çµæœè¡¨ç¤º
            st.subheader("ğŸ“ æ–‡å­—èµ·ã“ã—çµæœ")
            st.text_area("æ–‡å˜ä½ã§ã®æ–‡å­—èµ·ã“ã—", formatted_text, height=300)
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            st.download_button(
                label="ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=formatted_text,
                file_name=f"transcription_{os.path.splitext(uploaded_file.name)[0]}.txt",
                mime="text/plain"
            )
            
            # è¨€èªæ¤œå‡ºçµæœã®è¡¨ç¤ºï¼ˆè‡ªå‹•æ¤œå‡ºã®å ´åˆï¼‰
            if language == "":
                st.info(f"æ¤œå‡ºã•ã‚ŒãŸè¨€èª: {result['language']} (ä¿¡é ¼åº¦: {result['language_probability']:.1%})")
            
        except RuntimeError as e:
            if "CUDA out of memory" in str(e) or "DefaultCPUAllocator" in str(e):
                st.error(f"ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ã‹ã€è¨­å®šã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚\n\nã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
            else:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        except Exception as e:
            st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        
        finally:
            os.remove(tmp_path)
            display_memory_status()